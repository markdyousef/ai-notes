{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example: 'Hello World'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hello_constant = tf.constant('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### In TensorFlow, data isnâ€™t stored as integers, floats, or strings. These values are encapsulated in an object called a **tensor**.\n",
    "\n",
    "##### Constant Vector\n",
    "\n",
    "The values of a constant vector  (`tf.constant(array)`) never changes.\n",
    "\n",
    "##### Session\n",
    "\n",
    "Environment for running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.constant(1234)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=(3,) dtype=int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = tf.constant([123, 456, 789])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_3:0' shape=(2, 3) dtype=int32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.constant([[123, 456, 789], [222, 333, 444]])\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Placeholder\n",
    "`tf.placeholder()` returns a tensor that gets its value from data passed to the `tf.session.run()` function, allowing you to set the input right before the session runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 1, z: 0.54})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Quiz: Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    output = None\n",
    "    x = tf.placeholder(tf.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Feed the x tensor 123\n",
    "        output = sess.run(x, feed_dict={x: 123})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorFlow Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Addition\n",
    "Take in two numbers, two tensors, or one of each, and returns their sum as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.add(5,2) # 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Subtraction and Multiplication\n",
    "The `x` tensor will evaluate to `6`, because `10 - 4 = 6`. The `y` tensor will evaluate to `10`, because `2 * 5 = 10`. That was easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.subtract(10, 4) # 6\n",
    "y = tf.multiply(2, 5) # 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Converting types\n",
    "Convert between types to make certain operators work together.\n",
    "\n",
    "    tf.subtract(tf.constant(2.0),tf.constant(1))  # Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32:\n",
    "    \n",
    "That's because the constant `1` is an integer but the constant `2.0` is a floating point value and subtract expects them to match.\n",
    "\n",
    "In cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type. In this case, converting the `2.0` to an integer before subtracting, like so, will give the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sub_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1)) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Quiz: TF Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.divide(x,y) - 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TF Linear Function\n",
    "        y = xW + b\n",
    "`W` is a matrix of weights connecting two layers.\n",
    "\n",
    "The output `y`, the input `x`, and the biases `b` are all vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable\n",
    "The `tf.Variable` class creates a tensor with an initial value that can be modified - similar to a normal Python variable.\n",
    "This tensor stores its state in the **session**, so the state of the tensor needs to be initialized manually.\n",
    "* `tf.global_variables_initializer()` - returns an operation that initializes all TF variables from the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(5)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Weights\n",
    "It is a good practice to initialize the weights (W) with **random numbers from a normal distribution**.\n",
    "\n",
    "* **Randomizing** the weights helps the model from beoming stuck in the same place every time trained (Gradient Descent)\n",
    "* **Normal Distribution** prevents any one weight overwhelming other weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "\n",
    "# tensor with random values froma normal distribution,\n",
    "# whose magnitude is no more than 2 std from the mean\n",
    "weigts = tf.Variable(tf.truncated_normal((n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since the weights are already helping prevent the model from getting stuck,\n",
    "# in practice we don't randomize the bias\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # Linear Function (xW + b)\n",
    "    # (input*w+b)\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Loss: 7.926753997802734\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # We're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # Initialize session variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "The softmax function squashes it's inputs, typically called **logits** or **logit scores** to be:\n",
    "* between 0 and 1\n",
    "* normalize the outputs, so thay they all sum to 1\n",
    "\n",
    "The output is therefor equivalent to a **categorical probability distribution**>\n",
    "\n",
    "It's the perfect output activation for a network predicting multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65900117,  0.24243298,  0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "logit_data = [2.0, 1.0, 0.1]\n",
    "logits = tf.placeholder(tf.float32)\n",
    "\n",
    "# Calculate the softmax of the logits\n",
    "softmax =  tf.nn.softmax(logits)   \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Feed in the logit data\n",
    "    output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce Sum and Nutural Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes an array of numbers and sums them together\n",
    "a = tf.reduce_sum([1,2,3,4,5]) # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes the natural log of a number\n",
    "b = tf.log(100.0) # 4.60517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# Print cross entropy from session\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy,\n",
    "                   feed_dict={softmax: softmax_data,\n",
    "                              one_hot: one_hot_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "A technique for training on subsets of the dataset instead of all the data at one time.\n",
    "* don't have to store the entire dataset in memory\n",
    "* computationally inefficient, since the loss can't be calculated simultaneously accross all samples\n",
    "\n",
    "Usefull combined with SGD:\n",
    "1. randomly shuffle the data at the start of each epoch\n",
    "2. Create mini-batches\n",
    "3. Each minibatch's weights are trained with gradient descent. \n",
    "    * Since batches are random, SGD is performed on each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST totcal classes (0-9 digits)\n",
    "\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([n_inputs, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory\n",
    "Calculate the memory size of `train_features`, `train_labels`, `weights` and `bias` in bytes.\n",
    "* Calculate memory required for stored data (ignore memory for overhead)\n",
    "\n",
    "###### Single-precision floating-point format (float32)\n",
    "A computer number format that occupies 4 bytes (32 bits) in computer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows, columns = train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features memory 172480.0 Kb\n"
     ]
    }
   ],
   "source": [
    "train_features_memory = rows * columns * 4\n",
    "print('train_features memory {} Kb'.format(train_features_memory * 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels memory 2200.0 Kb\n"
     ]
    }
   ],
   "source": [
    "rows, columns = train_labels.shape\n",
    "\n",
    "train_labels_memory = rows * columns * 4\n",
    "print('train_labels memory {} Kb'.format(train_labels_memory * 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_memory memory 31.36 Kb\n"
     ]
    }
   ],
   "source": [
    "rows, columns = (n_inputs, c_classes)\n",
    "\n",
    "weights_memory = rows * columns * 4\n",
    "print('weights_memory memory {} Kb'.format(weights_memory * 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias_memory 0.04 Kb\n"
     ]
    }
   ],
   "source": [
    "bias_memory = c_classes * 4\n",
    "print('bias_memory {} Kb'.format(bias_memory * 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Mini-batching\n",
    "Divide data into batches\n",
    "* Since batches can vary in size we take advantage of `tf.placeholder()`\n",
    "\n",
    "The `None` dimension is a placeholcer for the `batch_size`. At runtime, TF will accept any batch size greater than `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#batches: 391\n",
      "last batch size: 80.0\n"
     ]
    }
   ],
   "source": [
    "features = (50000, 400)\n",
    "labels = (50000, 10)\n",
    "batch_size = 128\n",
    "\n",
    "batches = features[0] / batch_size\n",
    "\n",
    "print('#batches: {}'.format(round(batches)))\n",
    "print('last batch size: {}'.format((batches - int(batches)) * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    output_batches = []\n",
    "    sample_size = len(features)\n",
    "    \n",
    "    for start_index in range(0, sample_size, batch_size):\n",
    "        end_index = start_index + batch_size\n",
    "        batch = [features[start_index:end_index],\n",
    "                 labels[start_index:end_index]]\n",
    "        output_batches.append(batch)\n",
    "    \n",
    "    return output_batches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['F11', 'F12', 'F13', 'F14'],\n",
       "   ['F21', 'F22', 'F23', 'F24'],\n",
       "   ['F31', 'F32', 'F33', 'F34']],\n",
       "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
       " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches(3, example_features, example_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy: 0.14920000731945038\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "batch_size = 20\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "# logits: xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# loss and optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                            labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# calculate accuracy\n",
    "pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer,\n",
    "                 feed_dict={features: batch_features,\n",
    "                            labels: batch_labels})\n",
    "        \n",
    "    # calc accuracy for test dataset\n",
    "    test_accuracy = sess.run(accuracy,\n",
    "                             feed_dict={features: test_features,\n",
    "                                        labels: test_labels})\n",
    "\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs\n",
    "A single forward and backward pass of the entire dateset.\n",
    "* increase accuracy of the model without requiring more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud70/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud70/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud70/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud70/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "mnist = input_data.read_data_sets('datasets/ud70/mnist', one_hot=True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0    - Cost: 14.0     Valid Accuracy: 0.0768\n",
      "Epoch: 1    - Cost: 12.6     Valid Accuracy: 0.085\n",
      "Epoch: 2    - Cost: 11.6     Valid Accuracy: 0.0968\n",
      "Epoch: 3    - Cost: 10.8     Valid Accuracy: 0.109\n",
      "Epoch: 4    - Cost: 10.1     Valid Accuracy: 0.124\n",
      "Epoch: 5    - Cost: 9.5      Valid Accuracy: 0.138\n",
      "Epoch: 6    - Cost: 8.95     Valid Accuracy: 0.152\n",
      "Epoch: 7    - Cost: 8.46     Valid Accuracy: 0.168\n",
      "Epoch: 8    - Cost: 8.02     Valid Accuracy: 0.182\n",
      "Epoch: 9    - Cost: 7.63     Valid Accuracy: 0.199\n",
      "Test Accurracy: 0.1964000016450882\n"
     ]
    }
   ],
   "source": [
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                            labels=labels))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "def print_stats(epoch, sess, last_features, last_labels):\n",
    "    current_cost = sess.run(cost,\n",
    "                            feed_dict={\n",
    "                                features: last_features,\n",
    "                                labels: last_labels\n",
    "                            })\n",
    "    \n",
    "    valid_accuracy = sess.run(accuracy,\n",
    "                              feed_dict={\n",
    "                                  features: valid_features,\n",
    "                                  labels: valid_labels\n",
    "                              })\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate\n",
    "            }\n",
    "            \n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "        print_stats(epoch, sess, batch_features, batch_labels)\n",
    "    \n",
    "    test_accuracy = sess.run(accuracy, \n",
    "                             feed_dict={\n",
    "                                 features: test_features,\n",
    "                                 labels: test_labels\n",
    "                             })\n",
    "\n",
    "print('Test Accurracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
