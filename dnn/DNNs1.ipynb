{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Neural Networks (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Perceptrons as Logical Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### AND Perceptron\n",
    "<img src='//aind-notes.s3-website-eu-west-1.amazonaws.com/img/and-perceptron.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -1.0\n",
    "\n",
    "x = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [False, False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, True]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = x.dot([weight1, weight2]) + bias\n",
    "[x > 0 for x in output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### OR Perceptron\n",
    "<img src='//aind-notes.s3-website-eu-west-1.amazonaws.com/img/or-perceptron.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weight1 = 2.0\n",
    "weight2 = 1.0\n",
    "bias = 0.\n",
    "\n",
    "x = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [False, True, True, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, True]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = x.dot([weight1, weight2]) + bias\n",
    "[x > 0 for x in output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NOT Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weight1 = 1.0\n",
    "weight2 = -4.0\n",
    "bias = 2.0\n",
    "\n",
    "x = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [True, False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, True, False]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = x.dot([weight1, weight2]) + bias\n",
    "[x > 0 for x in output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XOR Perceptron\n",
    "<img src='//aind-notes.s3-website-eu-west-1.amazonaws.com/img/xor-perceptron.png'>\n",
    "<img src='//aind-notes.s3-website-eu-west-1.amazonaws.com/img/xor-nn.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Perceptron Algorithm\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/pa-pseudo.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i], W, b)\n",
    "        # pred is neg, but pos label\n",
    "        if y[i] - y_hat == 1:\n",
    "            # add lr*X to W\n",
    "            W += X[i].reshape(2,1) * learn_rate\n",
    "            # add lr to b\n",
    "            b += learn_rate\n",
    "        # pred is pos, but neg label\n",
    "        elif y[i] - y_hat == -1:\n",
    "            # sub lr*X from W\n",
    "            W -= X[i].reshape(2,1) * learn_rate\n",
    "            # sub lr from b\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/perceptron.csv', header=None).values\n",
    "X = data[:,:-1]\n",
    "y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.51307359]), array([ 0.70672918]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary_lines = trainPerceptronAlgorithm(X, y)\n",
    "boundary_lines[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Error Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Discrete vs. Continouos\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/disc-vs-con.png\">\n",
    "Since the steps in our error (loss, cost) functions are calculated by taking derivatives it has to be continous and differentiable. -> small variations in position will transform to small variations in height (mountain).\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/logloss1.png\" style=\"height: 150px; display: inline-block;\"/>\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/logloss2.png\" style=\"height: 150px; display: inline-block\" />\n",
    "\n",
    "We use **Gradient Descent** to gradually reduce the error.\n",
    "\n",
    "\n",
    "#### Predictions\n",
    "Continous error functions are better, when it comes to optimizing. For this we also need to make continous predictions.\n",
    "\n",
    "* Discrete predictions is Yes or No\n",
    "* Continous predictions is a number (e.g. probability)\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/disc-vs-con1.png\" />\n",
    "\n",
    "The way to move from discrete to continoues predictions is to change our **activation function** from our previous **step function** (perceptron) to e.g. a **sigmoid function**.\n",
    "\n",
    "We simply combine our linear function `Wx+b` with our sigmoid function:\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/activation.png\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/preds.png\" style=\"height: 200px; display: inline-block;\"/>\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/perceptron-sigmoid.png\" style=\"height: 200px; display: inline-block\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Multi-Class Classification\n",
    "* Equivalent to an activation function\n",
    "* Probabilities `sum` to 1\n",
    "* Softmax uses `exp()` to deal with negative numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input is a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    total = np.sum(np.exp(L))\n",
    "    probs = [i/total for i in np.exp(L)]\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-Hot Encoding\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/onehotenc.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Maximum Likelihood\n",
    "Pick the model that gives the existing labels the highest probabilities, thus by maximizing the probability we can pick the best model.\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/maxlike.png\"/>\n",
    "The better model gives better probabilities. How do we maximize these probabilities?\n",
    "With the right **error function**, minimizing the error is equivalent to maximizing the correct probabilities.\n",
    "\n",
    "But, calculating the best model by using products `(0.7*0.9*...)` isn't optimal for different reasons:\n",
    "* hard to calculate for large amount of probabilities\n",
    "* the product will be very tiny\n",
    "* a change in one number might change the product drastically\n",
    "\n",
    "Instead we want to `sum` over probabilities and for that we use `ln()` (natural log):\n",
    "`ln(ab) = ln(a) + ln(b)`\n",
    "\n",
    "To get positive numbers we use `-ln()`. The sum over all `-ln(p1), ...-ln(pn)` is called **Cross-Entropy**\n",
    "\n",
    "\n",
    "A good model will result in a low cross-entropy:\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/cross-ent.png\" />\n",
    "Calculating the ln for each point is equivalent to the error of each point. The points that are missclassified has larger values (errors).\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/cross-ent1.png\" />\n",
    "Therefor, our goal changes from maximizing probability to minimizing error in order to obtain the best model! And the error function we'll use is **Cross-Entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Cross-Entropy\n",
    "If we have a number of events and probabilities. How likely is it for the events to happen based on the probabilities? If it's very likely, the the cross-entropy is small, otherwise if it's unlikely, cross-entropy is large.\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/cross-ent2.png\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/cross-ent3.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.array(Y)\n",
    "    P = np.array(P)\n",
    "    errors = Y * np.log(P) + (1-Y) * np.log(1-P)\n",
    "    return -np.sum(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "P = [1, 1, 0]\n",
    "Y = [0.8, 0.7, 0.1]\n",
    "# passed the test, but kernel has issues with ln(0)\n",
    "# cross_entropy(Y,P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Multi-Class Cross-Entropy\n",
    "<img src=\"img/cross-ent-m.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6. Logistic Regression\n",
    "1. Take your data\n",
    "2. Pick a random model\n",
    "3. Calculate the error\n",
    "4. Minimize the error and obtain a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculating the Error Function\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/errfn.png\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/errfn1.png\" style=\"display: inline-block; height: 185px\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/errfn2.png\" style=\"display: inline-block; height: 185px\" />\n",
    "### Minimizing the Error Function\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/minerrfn.png\" />\n",
    "We'll gradually minimize the error function using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 7. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/gd.png\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/gd1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradient Calculation\n",
    "\n",
    "#### Sigmoid\n",
    "The sigmoid function has a really nice derivative:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma'(x) = \\sigma(x)(1-\\sigma(x))\n",
    "\\end{equation}\n",
    "\n",
    "The reason for this is the following, we can calculate it using the quotient formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma'(x) = \\frac{\\partial}{\\partial x} \\frac{1}{1+e^{-x}} \\\\\n",
    "           = \\frac{e^{-x}}{(1+e^{-x})^2} \\\\\n",
    "           = \\frac{1}{1+e^{-x}} * \\frac{e^{-x}}{1+e^{-x}} \\\\\n",
    "           = \\sigma(x)(1-\\sigma(x))\n",
    "\\end{equation}\n",
    "\n",
    "#### Error\n",
    "if we have `m` points labelled `x1, x2, ... xm` the error formula is:\n",
    "\n",
    "\\begin{equation}\n",
    "E = -\\frac{1}{m} \\sum_{i=1}^m (y_iln(\\hat y_i) + (1-y_i)ln(1-\\hat y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Where the prediction is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat y_i = \\sigma(Wx^{(i)} + b)\n",
    "\\end{equation}\n",
    "\n",
    "Our **goal** is to calculate the gradient of `E`, at a point `x=(x1,...,xm)`, given by the partial derivatives:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla E = (\\frac{\\partial}{\\partial w_1}E, ... , \\frac{\\partial}{\\partial w_n}E, \\frac{\\partial}{\\partial b}E)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "1. We'll calculate the derivative of the prediction:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial w_j}\\hat y = \\frac{\\partial}{\\partial w_j}{\\sigma(Wx + b)} \\\\\n",
    "= \\sigma(Wx + b)(1 - \\sigma(Wx + b)) * \\frac{\\partial}{\\partial w_j}(Wx + b) \\\\\n",
    "= \\hat y(1-\\hat y) * \\frac{\\partial}{\\partial w_j}(Wx + b) \\\\\n",
    "= \\hat y(1-\\hat y) * \\frac{\\partial}{\\partial w_j}(w_1x_1 + ... + w_jx_j + ... + w_nx_n + b) \\\\\n",
    "= \\hat y(1-\\hat y) * x_j\n",
    "\\end{equation}\n",
    "\n",
    "2. Now we can calculate the derivative of the Error:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial w_j}E = \\frac{\\partial}{\\partial w_j} (-\\frac{1}{m} \\sum_{i=1}^m (y_iln(\\hat y_i) + (1-y_i)ln(1-\\hat y_i)) \\\\\n",
    "= -\\frac{1}{m}\\sum_{i=1}^m y_i\\frac{\\partial}{\\partial w_j}ln(\\hat y_i) + (1-y_i)\\frac{\\partial}{\\partial w_j}ln(1-\\hat y_i) \\\\\n",
    "= -\\frac{1}{m}\\sum_{i=1}^m y_i\\frac{1}{\\hat y_i}\\frac{\\partial}{\\partial w_j}\\hat y_i + (1-y_i)\\frac{-1}{1-\\hat y_i}\\frac{\\partial}{\\partial w_j}\\hat y_i \\\\\n",
    "= -\\frac{1}{m}\\sum_{i=1}^m y_i\\frac{1}{\\hat y_i}\\hat y_i(1-\\hat y_i)x^{(i)}_j + (1-y_i)\\frac{-1}{1-\\hat y_i}\\hat y_i(1-\\hat y_i)x^{(i)}_j\\\\\n",
    "= -\\frac{1}{m}\\sum_{i=1}^m y_i(1-\\hat y_i)x^{(i)}_j - (1-y_i)\\hat y_i x^{(i)}_j\\\\\n",
    "= -\\frac{1}{m}\\sum_{i=1}^m (y_i - \\hat y_i)x^{(i)}_j\n",
    "\\end{equation}\n",
    "\n",
    "A similar calculation will show that:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial b}E = -\\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat y_i)\n",
    "\\end{equation}\n",
    "\n",
    "For a point with coordinates `(x1,...,xn)`, label `y` and prediction `y_hat`, the gradient of the error function at that point is `((y - y_hat)x1,...,(y - y_hat)xn,(y - y_hat))`.\n",
    "In summary:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla E(W,b) = (y - \\hat y)(x_1,...,x_n, 1)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "    The gradient is actually a scalar times the coordinates of the point.\n",
    "    This scalar is the difference between the label and the prediction. If the label is close to the prediction (well classified) the gradient is small, else it is big.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradient Descent Algorithm\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/gd-alg.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "def prediction(X, W, b):\n",
    "    return sigmoid(np.matmul(X,W)+b)\n",
    "def error_vector(y, y_hat):\n",
    "    return [-y[i]*np.log(y_hat[i]) - (1-y[i])*np.log(1-y_hat[i]) for i in range(len(y))]\n",
    "def error(y, y_hat):\n",
    "    ev = error_vector(y, y_hat)\n",
    "    return sum(ev)/len(ev)\n",
    "\n",
    "# Calculate the gradient of the error function.\n",
    "# The result should be a list of three lists:\n",
    "# The first list should contain the gradient (partial derivatives) with respect to w1\n",
    "# The second list should contain the gradient (partial derivatives) with respect to w2\n",
    "# The third list should contain the gradient (partial derivatives) with respect to b\n",
    "def dErrors(X, y, y_hat):\n",
    "    errors = np.array([y[i] - y_hat[i] for i in range(len(y))])\n",
    "    DErrorsDx1 = X[0] * errors\n",
    "    DErrorsDx2 = X[1] * errors\n",
    "    DErrorsDb = errors\n",
    "    return DErrorsDx1, DErrorsDx2, DErrorsDb\n",
    "\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b.\n",
    "# It should calculate the prediction, the gradients, and use them to\n",
    "# update the weights and bias W, b. Then return W and b.\n",
    "# The error e will be calculated and returned for you, for plotting purposes.\n",
    "def gradientDescentStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Calculate the prediction\n",
    "    y_hat = prediction(X, W, b)\n",
    "    # Calculate the gradient\n",
    "    dx1, dx2, db = dErrors(X, y, y_hat)\n",
    "    # Update the weights\n",
    "    W += np.sum([dx1, dx2]) * learn_rate\n",
    "    b += np.sum(db) * learn_rate\n",
    "    # This calculates the error\n",
    "    e = error(y, y_hat)\n",
    "    return W, b, e\n",
    "\n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainLR(X, y, learn_rate = 0.01, num_epochs = 100):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    # Initialize the weights randomly\n",
    "    W = np.array(np.random.rand(2,1))*2 -1\n",
    "    b = np.random.rand(1)[0]*2 - 1\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    errors = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the gradient descent step.\n",
    "        W, b, error = gradientDescentStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "        errors.append(error)\n",
    "    return boundary_lines, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/perceptron.csv', header=None).values\n",
    "X = data[:,:-1]\n",
    "y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = trainLR(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
