{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Local Connectivity\n",
    "\n",
    "**Multilayer Perceptrons (MLPs):**\n",
    "* Only use fully connected layers\n",
    "* Only accept vectors as input\n",
    "\n",
    "**CNNs:**\n",
    "* Also use sparsely connected layers\n",
    "* Also accept matrices as input\n",
    "\n",
    "Locally connected layers (CNN) uses far fewer parameters then densely(fully) connected layers, less prune to overfitting and truly understands how to tease out the patterns contained in image data.\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/fcl.png\" style=\"display: inline-block; height: 180px\"/>\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/cl.png\" style=\"display: inline-block; height: 180px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Convolutional Layers\n",
    "Conv Layers break the image up into smaller pieces - convolution window (e.g. colored areas above).\n",
    "We slide the **window** horizontally and vertically over the matrix of image pixels.\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/convlayer.png\" style=\"height: 300px\"/>\n",
    "Instead of representing the weights in the arrows (as above) we'll use a matrix. This weight matrix is called a **filter**. The size of the filter is the same as the window.\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/convlayer1.png\" style=\"height: 300px\"/>\n",
    "In conv layers we usually use multiple filters to detect distinct features of the input. The activation maps produced by applying different filters produceses simplifications of the input - only pays attention to small set of features.\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/convlayer2.png\" style=\"display: inline-block; height: 250px\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/convlayer3.png\" style=\"display: inline-block; height: 250px\"/>\n",
    "\n",
    "    The size(height x width) and number of filters are hyperparameters that can be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Stride and Padding\n",
    "The stride of a convolution is the amount by which the filters slide accross the image. With a stride of 1, the filters is slided 1px (horizontally and vertically) at a time, resulting in a conv layer of almost identical size of the input image.\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/stride.png\" style=\"height: 320px; display: inline-block\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/padding.png\" style=\"height: 320px; display: inline-block\" />\n",
    "\n",
    "It's common to add padding (zeros) around the edges of the image to avoid loosing information, when applying the filter (e.g. if filter size doesn't match input size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolutional Layers in Keras\n",
    "    from keras.layers import Conv2D\n",
    "    \n",
    "    Conv2D(filters, kernel_size, strides, padding, activation=\"relu\", input_shape)\n",
    "* `filters` - the number of filters\n",
    "* `kernel_size[number || tuple]` - height and width of the convolution window\n",
    "* `?strides[number || tuple]` - stride of the convolution (`default=1`)\n",
    "* `?padding` - `valid` or `same` (`default='valid'`)\n",
    "* `?activation` - typically use `relu` (`default=None`)\n",
    "* `?input_shape[tuble]` - only used if this layer is input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Dimensionality\n",
    "\n",
    "### Number of Parameters in a Convolutional Layer\n",
    "The number of parameters in a convolutional layer depends on the supplied values of `filters`, `kernel_size`, and `input_shape`. Let's define a few variables:\n",
    "\n",
    "* K - the number of filters in the convolutional layer\n",
    "* F - the height and width of the convolutional filters\n",
    "* D_in - the depth of the previous layer\n",
    "\n",
    "Notice that `K = filters`, and `F = kernel_size`. Likewise, `D_in` is the last value in the `input_shape` tuple.\n",
    "\n",
    "Since there are `F*F*D_in` weights per filter, and the convolutional layer is composed of K filters, the total number of weights in the convolutional layer is `K*F*F*D_in`. Since there is one bias term per filter, the convolutional layer has K biases. Thus, the number of parameters in the convolutional layer is given by ** `K*F*F*D_in + K` **.\n",
    "\n",
    "### Shape of a Convolutional Layer\n",
    "The shape of a convolutional layer depends on the supplied values of `kernel_size`, `input_shape`, `padding`, and `stride`. Let's define a few variables:\n",
    "\n",
    "* K - the number of filters in the convolutional layer\n",
    "* F - the height and width of the convolutional filters\n",
    "* S - the stride of the convolution\n",
    "* H_in - the height of the previous layer\n",
    "* W_in - the width of the previous layer\n",
    "\n",
    "Notice that `K = filters`, `F = kernel_size`, and `S = stride`. Likewise, `H_in` and `W_in` are the first and second value of the `input_shape` tuple, respectively.\n",
    "\n",
    "The **depth** of the convolutional layer will always equal the number of filters K.\n",
    "\n",
    "If `padding = 'same'`, then the spatial dimensions of the convolutional layer are the following:\n",
    "\n",
    "* **height** = ceil(float(`H_in`) / float(`S`))\n",
    "* **width** = ceil(float(`W_in`) / float(`S`))\n",
    "\n",
    "If `padding = 'valid'`, then the spatial dimensions of the convolutional layer are the following:\n",
    "\n",
    "* **height** = ceil(float(`H_in - F` + 1) / float(`S`))\n",
    "* **width** = ceil(float(`W_in - F` + 1) / float(`S`))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Pooling Layers\n",
    "Pooling layers often take conv layers as input. We often use many filters in the conv layers (big stack) which means that our dimensionality can get quite large. Higher dimensionality requires more parametes, which can lead to overfitting. \n",
    "\n",
    "Pooling layers (MaxPooling, Global Average Pooling, etc.) takes the feature map and reduces it to the most important features. \n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/pooling.png\" style=\"height: 320px; display: inline-block\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/pooling1.png\" style=\"height: 320px; display: inline-block\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Max Pooling Layers in Keras\n",
    "\n",
    "    from keras.layers import MaxPooling2D\n",
    "        \n",
    "    MaxPooling2D(pool_size, strides, padding)\n",
    "    \n",
    "* `pool_size[number || tuple]` - height and width of the pooling window\n",
    "* `?strides[number || tuple]` - vertical and horizontal stride (`default=pool_size`)\n",
    "* `?padding` - `valid` or `same` (`default='valid'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 15)        0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, input_shape=(100, 100, 15)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6. CNNs for Image Classification\n",
    "\n",
    "We're currently resizing input images to the same fixed size (e.g. 32x32x3). \n",
    "\n",
    "Our CNN architecture is designed with the goal of decreasing the width and height of our input images, while increasing the depth.\n",
    "\n",
    "* Convolutional layers are used to make the input array deeper\n",
    "* Max Pooling layers are used to decrease the spatial dimensions: width and height \n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/cnn.png\" style=\"height: 250px; display: inline-block\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/cnn1.png\" style=\"height: 250px; display: inline-block\" />\n",
    "\n",
    "#### Things to Remember\n",
    "* Always add `'relu'` activation function to the `Conv2D` layers\n",
    "* `Dense` layers should also use `'relu'`, except for the final layer\n",
    "* Always use `'softmax'` in the final layer for multi-class classification with the total number of classes\n",
    "* We usually gradually increase the `filters` (depth) of the `Conv2D` layers `(16, 32, 64...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create(max_pooling=False):    \n",
    "    model = Sequential()\n",
    "    # These first six layers are designed to take the input array of image pixels\n",
    "    # and convert it to an array where all of the spatial information has been squeezed out,\n",
    "    # and only information encoding the content of the image remains\n",
    "    model.add(Conv2D(filters=16, kernel_size=2,\n",
    "                     padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "    if max_pooling==True: model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    if max_pooling==True: model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "    if max_pooling==True: model.add(MaxPooling2D(pool_size=2))\n",
    "    # The array is then flattened to a vector in the seventh layer of the CNN\n",
    "    model.add(Flatten())\n",
    "    # It is followed by two dense layers designed to further elucidate the content\n",
    "    # of the image.\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    # The final layer has one entry for each object class in the dataset,\n",
    "    # and has a softmax activation function, so that it returns probabilities\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 16)        208       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 32)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 64)        8256      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               32768500  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 32,784,054\n",
      "Trainable params: 32,784,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "no_max_pooling = create()\n",
    "no_max_pooling.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 16)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               512500    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 528,054\n",
      "Trainable params: 528,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_pooling = create(max_pooling=True)\n",
    "max_pooling.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Image Augmentation\n",
    "We want our algorithm to learn an **invariant representation** of the image. When working with image-data, we often encounter a lot of irrelavant data (noise) such as differences in width, height, color, position, etc. We just want to know if the image contains an advocado.  \n",
    "* Scale invariance: we don't want our model to change it's prediction due to the size of the object\n",
    "* Rotation invariance: we don't want the angle to matter\n",
    "* Translation invariance: we don't want the position to matter\n",
    "\n",
    "CNNs has some build-in translation invariance (max-pooling).\n",
    "\n",
    "Technique for making our algorithms more statistically invariant. \n",
    "___\n",
    "\n",
    "**The idea is simple:**\n",
    "\n",
    "if you want your network to be more ***rotation invariant***:\n",
    "* add rotated augmentations to the training data, created by random rotations of it\n",
    "\n",
    "if you want your network to be more **translation invariant**:\n",
    "* add translated augmentations to the training data, cretad by random translations of it\n",
    "\n",
    "\n",
    "#### We expand the training set by augmenting the data\n",
    "Data augmentation also help us to avoid **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 7. GroundBreaking CNN Architectures\n",
    "\n",
    "* AlexNet: [paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "\n",
    "* VGGNet: [paper](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "\n",
    "* ResNet: [paper](https://arxiv.org/pdf/1512.03385v1.pdf)\n",
    "\n",
    "* Keras CNN: [examples](https://arxiv.org/pdf/1512.03385v1.pdf)\n",
    "\n",
    "* Vanishing Gradients: [blog](http://neuralnetworksanddeeplearning.com/chap5.html)\n",
    "\n",
    "* Benchmarks: [github](https://github.com/jcjohnson/cnn-benchmarks)\n",
    "\n",
    "* ImageNet Large Scale Visual Recognition Competition (ILSVRC): [page](http://www.image-net.org/challenges/LSVRC/)\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/alexnet.png\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/vgg.png\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/resnet.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 8. Visualizing CNNs\n",
    "If we can learn to better understand how a CNN learns, we can help it to perform better.\n",
    "\n",
    "* Visualizing how CNNs learn: [page](http://cs231n.github.io/understanding-cnn/)\n",
    "\n",
    "* App that visualizes CNNs in real-time: [demo](http://cs231n.github.io/understanding-cnn/) [page](http://openframeworks.cc/)\n",
    "\n",
    "    * other visualization tool: [demo](https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=78s) [video](https://www.youtube.com/watch?v=ghEmQSxT6tw&t=5s)\n",
    "\n",
    "    * other visualization tool: [page](https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5)\n",
    "\n",
    "* Keras blog post *#DeepDreams*: [blog](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) [video](https://www.youtube.com/watch?v=XatXy6ZhKZw) [demo](https://deepdreamgenerator.com/)\n",
    "\n",
    "* Interpretability of CNNs: [article](https://blog.openai.com/adversarial-example-research/) [paper](https://arxiv.org/abs/1611.03530)\n",
    "\n",
    "\n",
    "#### ImageNet\n",
    "* ressources: [paper](http://www.matthewzeiler.com/pubs/arxive2013/eccv2014.pdf)\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/layer2.png\" style=\"height: 200px\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/layer3.png\" style=\"height: 200px\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/layer5.png\" style=\"height: 400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 9. Transfer Learning\n",
    "___\n",
    "Use the learned understanding from previosly trained models and pass it on to our new deep learning model.\n",
    "\n",
    "### Transfer learning involves taking a **pre-trained** neural network and adapting the neural network to a new, different data set.\n",
    "\n",
    "Depending on both:\n",
    "* the size of the new data set, and\n",
    "* the similarity of the new data set to the original data set\n",
    "\n",
    "...the approach for using transfer learning will be different. \n",
    "#### There are four main cases:\n",
    "\n",
    "1. new data set is **small**, new data is **similar** to original training data\n",
    "2. new data set is **small**, new data is **different** from original training data\n",
    "3. new data set is **large**, new data is **similar** to original training data\n",
    "4. new data set is **large**, new data is **different** from original training data\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer.png\" style=\"height: 400px;\" />\n",
    "\n",
    "\n",
    "* the dividing line between a large and small data set is somewhat subjective\n",
    "* overfitting is a concern when using transfer learning with a small data set.\n",
    "* images of dogs and images of wolves would be considered similar (common characteristics)\n",
    "* images of flowers and images of dogs would be considered different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Demonstration\n",
    "___\n",
    "#### Ressources\n",
    "* Systematic analyzis of transferability of features: [paper](https://arxiv.org/pdf/1411.1792.pdf)\n",
    "\n",
    "* Sebastian Thrun's cancer-detecting CNN: [article](http://www.nature.com/articles/nature21056.epdf?referrer_access_token=_snzJ5POVSgpHutcNN4lEtRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuP9jVts1q2g1KBbk3Pd3AelZ36FalmvJLxw1ypYW0UxU7iShiMp86DmQ5Sh3wOBhXDm9idRXzicpVoBBhnUsXHzVUdYCPiVV0Slqf-Q25Ntb1SX_HAv3aFVSRgPbogozIHYQE3zSkyIghcAppAjrIkw1HtSwMvZ1PXrt6fVYXt-dvwXKEtdCN8qEHg0vbfl4_m&tracking_referrer=edition.cnn.com)\n",
    "\n",
    "* Object localization: [paper](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)\n",
    "[repo](https://github.com/alexisbcook/ResNetCAM-keras)\n",
    "[video](https://www.youtube.com/watch?v=fZvOy0VXWAI)\n",
    "\n",
    "* Bottleneck features: [repo](https://github.com/alexisbcook/keras_transfer_cifar10)\n",
    "\n",
    "\n",
    "___\n",
    "#### Overview\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer2.png\" style=\"height: 400px;\" />\n",
    "* the first layer will detect edges in the image\n",
    "* the second layer will detect shapes\n",
    "* the third layer will detects higher level features\n",
    "___\n",
    "#### Case 1: Small Data Set, Similar Data:\n",
    "\n",
    "To **avoid overfitting** on the small dataset, the weights of the original network will be held constant.\n",
    "\n",
    "Since the data sets are similar, images from each data set will have similar higher level features. The pre-trained network contains relevant information about the new data set.\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer3.png\" style=\"height: 150px; display: inline-block; margin: auto\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer4.png\" style=\"height: 340px; display: inline-block; margin: auto\" />\n",
    "\n",
    "###### Approach:\n",
    "\n",
    "1. slice off the end of the neural network\n",
    "2. add a new **fully connected** layer that matches the number of classes in the new data set\n",
    "3. **randomize** the weights of the new fully connected layer; freeze all the weights from the pre-trained network\n",
    "4. train the network to update the weights of the new fully connected layer\n",
    "___\n",
    "#### Case 2: Small Data Set, Different Data\n",
    "To **avoid overfitting** on the small dataset, the weights of the original network will be held constant.\n",
    "\n",
    "Since the data sets are different, they don't share higher level features. We'll only use the low-lewel features from the pre-trained model.\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer5.png\" style=\"height: 150px; display: inline-block; margin: auto\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer6.png\" style=\"height: 340px; display: inline-block; margin: auto\" />\n",
    "\n",
    "###### Approach:\n",
    "1. slice off most of the pre-trained layers near the beginning of the network\n",
    "2. add a new **fully connected** layer that matches the number of classes in the new data set\n",
    "3. **randomize** the weights of the new fully connected layer; freeze all the weights from the pre-trained network\n",
    "4. train the network to update the weights of the new fully connected layer\n",
    "\n",
    "___\n",
    "#### Case 3: Large Data Set, Similar Data\n",
    "Overfitting is not as much of a concern when training on a large data set; therefore, you can re-train all of the weights.\n",
    "\n",
    "Since the data sets are similar, images from each data set will have similar higher level features. The pre-trained network contains relevant information about the new data set.\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer7.png\" style=\"height: 150px; display: inline-block; margin: auto\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer8.png\" style=\"height: 340px; display: inline-block; margin: auto\" />\n",
    "\n",
    "###### Approach:\n",
    "1. remove the last fully connected layer and replace with a layer matching the number of classes in the new data set\n",
    "2. randomly initialize the weights in the new fully connected layer\n",
    "3. initialize the rest of the weights using the pre-trained weights\n",
    "4. re-train the entire neural network\n",
    "\n",
    "___\n",
    "#### Case 4: Large Data Set, Different Data\n",
    "Overfitting is not as much of a concern when training on a large data set; therefore, you can re-train all of the weights.\n",
    "\n",
    "\n",
    "Even though the data set is different from the training data, initializing the weights from the pre-trained network might make training faster. So this case is exactly the same as the case with a large, similar data set\n",
    "\n",
    "If using the pre-trained network as a starting point does not produce a successful model, another option is to randomly initialize the convolutional neural network weights and train the network from scratch.trans\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer9.png\" style=\"height: 150px; display: inline-block; margin: auto\" />\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/transfer10.png\" style=\"height: 340px; display: inline-block; margin: auto\" />\n",
    "\n",
    "###### Approach:\n",
    "1. remove the last fully connected layer and replace with a layer matching the number of classes in the new data set\n",
    "\n",
    "\n",
    "* retrain the  network from scratch with randomly initialized weights\n",
    "* alternatively, you could just use the same strategy as \"Large Data Set, Similar Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
