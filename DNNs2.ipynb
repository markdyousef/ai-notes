{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Neural Networks (L2, part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Neural Network Architecture\n",
    "Combining simple linear models (perceptrons) into more complicated non-linear models.\n",
    "We can also weight the different models and assign higher relevance to certain model/ layer outputs.\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/nna.png\" style=\"display: inline-block; height: 180px\">\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/nna1.png\" style=\"display: inline-block; height: 180px\">\n",
    "This is similar to the simple models from earlier - a linear combination of input values, times the weights plus some bias `(y = xW+b)`. Now the model is a linear combination of the previous models output, times the weights and plus some bias.\n",
    "\n",
    "In essence Neural Networks are composed from simpler linear models into more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Feedforward\n",
    "The process neural networks use to turn the input into output. Training neural networks is choosing parameters on the network edges in order to model the data well.\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/ffn.png\" style=\"display: inline-block; height: 200px\">\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/ffn1.png\" style=\"display: inline-block; height: 200px\">\n",
    "    \n",
    "    Here the bias is threated as a weight and input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Backpropagation\n",
    "In a nutshell:\n",
    "* Doing a feedforward operation\n",
    "* Comparing the output of the model with the desired output\n",
    "* Calculating the error\n",
    "* Running a feedforward backwards (backprop) to spread the error of each of the weights and bias\n",
    "* Update the weights and get a better model\n",
    "* Continue until our model is good (convergence)\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/backp.png\" style=\"height: 300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Backpropagation math\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/backp1.png\" style=\"display: inline-block; height: 200px\">\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/backp2.png\" style=\"display: inline-block; height: 200px\">\n",
    "\n",
    "        E is a vector\n",
    "#### Chain Rule\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/chainrule.png\" style=\"height: 300px\">\n",
    "\n",
    "#### Process\n",
    "First we perform a feedforward operation. (Here the bias is interpreted as a weight for convenience, such that it can fit in a matrix).\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/backprop.png\" style=\"height: 300px\">\n",
    "We compare the output of the model with the desired output, by calculating the partial derivatives of the error function with respect to each weight (only W_11 is shown) using the chain rule.\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/backprop1.png\" style=\"display: inline-block; height: 200px\">\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/backprop2.png\" style=\"display: inline-block; height: 200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Training Optimization\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "We take smaller subsets of the data, run them through the neural network, calculate the error function based on that subset.\n",
    "We still want to use all the data, so we split it into separate batches and calculate the gradients for each. \n",
    "Splitting the data into batches and calculated the gradient on each, might be less accurate then calculating the gradient for the entire data set. But it is much faster and in practice, accurate enough.\n",
    "\n",
    "### Learning Rate\n",
    "If the learning rate is big, the optimizer is taking large steps. When it's small, the optimizer is taking small steps. Usually, we want to take larger steps at the beginning and smaller steps at the end. \n",
    "\n",
    "A general rule of thumb is, that if the model isn't working try to decrease the learning rate.\n",
    "\n",
    "### Split data into train, test and validation set\n",
    "Helps aliviate overfitting and underfitting, by separating the data into different sets to evaluate the model.\n",
    "\n",
    "\n",
    "    Usually we want to try to overfit our model and then apply different techniques to try to generalize better.\n",
    "    \n",
    "### Early Stopping\n",
    "<img src=\"img/earlystopping.png\">\n",
    "<img src=\"img/earlystopping1.png\">\n",
    "\n",
    "### Regularization (L1 and L2)\n",
    "Regularization punishes large weights (coefficients):\n",
    "\n",
    "<img src=\"img/regularization.png\" style=\"display: inline-block; height: 200px\">\n",
    "<img src=\"img/regularization1.png\" style=\"display: inline-block; height: 200px\">\n",
    "Both L1 and L2 and popular and should be chosen depending on the problem:\n",
    "\n",
    "**L1**:\n",
    "* Tend to end up with sparse vectors `(1, 0, 0, 1, 0)` -> small weights will tend to go to 0. If we want to reduce the number of weights and end up with a small set.\n",
    "* Good for feature selection\n",
    "\n",
    "**L2**:\n",
    "* Tend not to favor sparse vectors `(0.5, 0.3, -0.2, 0.4, 0.1)`, since it tries to maintain all weights homogeniusly small.\n",
    "* Normally better for training models\n",
    "\n",
    "### Dropout\n",
    "Sometimes training our neural networks, some parts of the network ends up dominating with very large weights whereas others don't. To solve this, we randomly turn some nodes of the network on and off during epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Vanishing Gradient (chaning the Activation Function)\n",
    "With the sigmoid function, the derivatives can get very small making it harder to train our model.\n",
    "\n",
    "To fix this, we can change the activation function:\n",
    "\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/tanh.png\" style=\"display: inline-block; height: 250px\">\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/relu.png\" style=\"display: inline-block; height: 250px\">\n",
    "\n",
    "* tanh is very similar to sigmoid, but since the range is between -1 and 1, the gradients are larger.\n",
    "* ReLU returns the same value of positive and 0 if negative (max between x and 0). The derivative is 1 if the number is positive - improves training time significantly without sacrificing accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Local Minima\n",
    "Techniques for avoiding getting stuck in a local minima:\n",
    "* Random Restarts\n",
    "* Change optimizer, e.g. momentum, adam, adagrad, etc.\n",
    "<img src=\"//aind-notes.s3-website-eu-west-1.amazonaws.com/img/momentum.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
